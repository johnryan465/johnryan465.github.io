[{"content":"Matrix Multiplication is something which you may never really have considered too much, it is how we calculate the composition of 2 linear transformations. The algorithm is pretty simple and doesn\u0026rsquo;t seem inefficient in any real way. However there is quite a bit more than meets the eye to this.\nDefinition We will be considering the following linear maps\n$L_A : W \\rightarrow U, L_B : V \\rightarrow W, L_C : V \\rightarrow U$\n$n = dim(W), m = Dim(V), p= \\dim(U)$\nWe want to find $L_C = L_A \\circ L_B$, in particular we actually want to compute the matrix $C$ that represents $L_C$ from the matrices $A,B$.\nStarting point The method for matrix multiplication we all know is as follows.\n$c_{ij} = \\sum_{k=1}^n a_{ik}b_{kj}$\nTo compute each entry of $C$, we perform $n$ multiplications and $n-1$ additions. We must do this for all $m \\times p$ entries of $C$. So we have $npm$ multiplications and $(n-1)mp$ additions.\nThis leads to an algorithm with computational complexity of $O(n^3)$ for multiplication of 2 $n \\times n$ matrices.\nThere doesn\u0026rsquo;t seem to really be any redundant calculations, but actually improvements on this exist.\nStratten\u0026rsquo;s Algorithm Strattens Algorithm was the first sub-cubic complexity algorithm for matrix multiplication. We can write matrix multiplication as $8$ sub-matrix multiplications and $4$ matrix additions. This algorithm is also $O(n^3)$, we can quite easily prove this.\n$$\\begin{aligned}\\begin{bmatrix} C_{1,1} \u0026amp; C_{1,2} \\ C_{2,1} \u0026amp; C_{2,2} \\ \\end{bmatrix} \u0026amp;= \\begin{bmatrix} A_{1,1} \u0026amp; A_{1,2} \\ A_{2,1} \u0026amp; A_{2,2} \\end{bmatrix} \\begin{bmatrix} B_{1,1} \u0026amp; B_{1,2} \\ B_{2,1} \u0026amp; B_{2,2} \\end{bmatrix} \\ \u0026amp;= \\begin{bmatrix} A_{1,1}B_{1,1} + A_{1,2}B_{2,1} \u0026amp; A_{1,1}B_{1,2} + A_{1,2}B_{2,2} \\ A_{2,1}B_{1,1} + A_{2,2}B_{2,1} \u0026amp; A_{2,1}B_{1,1} + A_{2,2}B_{2,2} \\end{bmatrix} \\end{aligned}$$\nStrattens Algorithm works by instead of doing $8$ sub matrix multiplications it does $7$, this is in exchange for more additions.\nThe matrix multiplications are defined as follows.\n$$\\begin{aligned}M_1 \u0026amp;= (A_{1,1} + B_{2,2})(B{1,1} + B_{2,2}) \\M_2 \u0026amp;= (A_{2,1} + A_{2,2})B_{1,1} \\M_3 \u0026amp;= A_{1,1}(B_{1,2} - B_{2,2}) \\M_4 \u0026amp;= A_{2,2} (B_{2,1} - B_{2,2}) \\M_5 \u0026amp;= (A_{1,1} + A_{1,2})B_{2,2} \\M_6 \u0026amp;= (A_{2,1} - A_{1,1})(B_{1,1} + B_{1,2}) \\M_7 \u0026amp;= (A_{1,2} - A_{2,2})(B_{2,1} + B_{2,2})\\end{aligned}$$\nThe we have\n$$\\begin{aligned}C_{1,1} \u0026amp;= M_1 + M_4 - M_5 + M_7 \\C_{1,2} \u0026amp;= M_3 + M_5 \\C_{2,1} \u0026amp;= M_2 + M_4 \\C_{2,2} \u0026amp;= M_1 - M_2 + M_3 + M_6\\end{aligned}$$\nWe can verify that this holds by matrix algebra is quite easily, and them find that the complexity of the algorithm is $O(n^{\\log_2(7)})$.\nThis is \u0026hellip; deeply unsatisifiying, atleast to me. The seemingly arbitrary matrix products don\u0026rsquo;t really seem to give any insight as to what is occurring and why it is better than the naive approach.\nA linear transformation is completely defined by its actions on a basis, in this case the basis is of size $n$. Computing each of the transformations on the basis seperately takes $O(n^2)$ operations. So we are somehow taking advantage of some inherent structure in matrix multiplication to be able to calculate this using a sub-cubic algorithm.\nWe wish to determinine the smallest $\\omega$ such that matrix multiplication is $O(n^\\omega)$\nBut what structure? Is there similarly sequence of $6$ sub-matrix multiplications that we can use to create an even better matrix multiplication algorithm?\nTensors and Multi-linear Maps Matrices represent linear maps from one vector space to another. Matrix Multiplication takes 2 linear maps, and maps this to another one, lets call this mapping $MM$.\nMore formally.\n$L_A : W \\rightarrow U, L_B : V \\rightarrow W, L_C : V \\rightarrow U$\n$$MM : (W \\rightarrow U) \\times (V \\rightarrow W) \\rightarrow ( V \\rightarrow U)$$\nEach of these linear maps are themselves elements of a vector space.\n$$MM_{V,W,U} : A \\times B \\rightarrow C, A \\in L(W,U), B \\in L(V,W), C \\in L(V,U)$$\nWe have $\\lambda C = (\\lambda A)B = A(\\lambda B), \\lambda \\in F$. This means that our mapping $MM$ is bilinear in its 2 inputs.\nNote: From here on out we will pretty much be be working with the case of multiplying 2 n*n matrices over the same field.\nIn the spirit of this we can instead write our Tensor as $\\langle n \\rangle$ if all dimensions are equal or $\\langle m,n,p \\rangle$, where $F$ is our field and $m,n,p$ are the dimensions we are working over, we will call this space $V$\nTensors A bit of an introduction to tensors for anyone who hasn\u0026rsquo;t seen them before.\nWe define a tensor of $T$ over vector spaces ${V_1,\\ldots,V_n}$ over a common field $F$ as follows\n$$T : V_1 \\times \\ldots \\times V_n \\rightarrow F $$\nNote: more restrictive versions of this definition exist where the vector spaces used are restricted to multiple copies of a vector space and its dual\n$T$ is a multi-linear map, it is linear in each of its inputs.\nBy fixing a basis for ${V_1,\\ldots,V_n}$ we get a multidimensional array $d$ of dimensions $n$, with the entries of this array being elements of $F$.\n$$T = \\sum_{u=1}^{\\dim V_1} \\ldots \\sum_{w=1}^{\\dim V_n} d_{u \\ldots v} {x}_u \\otimes \\ldots \\otimes z_w$$.\nwhere ${x, \\ldots, z}$ are the corresponding basis for the vector spaces.\nUsing this form we can rewrite the matrix multiplication tensor as\n$$\\langle m,n,p \\rangle = \\sum_{i=1}^{m} \\sum_{j=1}^{p} \\sum_{k=1}^{n} a_{ik} \\otimes b_{kj} \\otimes c_{ij}$$\nWe have now shown that we can talk about matrix multiplication as a multi-linear map (bi-linear) and in turn a tensor, which is cool I guess but seems like quite a tangent.\nA theorem approaches The tensor representation seems a bit of of left field, however by using this we can bring some powerful maths tools to bear. But before we can do this, we unfortunately need more definitions.\nFirst we will need to define what the rank of a tensor is.\nIt is the minimum number of rank 1 tensors required to represent our tensor.\n$$rk(t) = min_R st \\sum^R_{r=1} \\lambda_r a_{1,r} \\otimes \\ldots a_{M,r} $$\nThe border rank is then minimum rank of a sequence of tensors of which the limit is is the tensor in question.\nClearly we have $brk(t) \\leq rk(t)$, as the sequence which is just the tensor with its rank $r$ decompisition repeated gives a feasible border rank of $r$\nWe also have the simple result that $rk(t) \\leq \\prod^M_{i=1} dim(V_i)$. We get this by creating a rank 1 tensor corresponding to the each of the basis vector of each constituent vector space.\nRestrictions Rank and border rank of tensors are unfortunately not (yet) as well understood as the rank of matrices. However even without complete results, we can make some progress.\nUseful facts  The number of elementary multiplications required for a bilinear map (which matrix multiplication is), is tightly bounded by the rank of the corresponding tensor. The rank of a tensor product is sub multiplicative.  The first of these is very useful, it shows that attempting to bound the rank of the matrix multiplication tensor is pretty much exactly what we want to do to determine $\\omega$.\nThe sub-multiplicative result implies that if we can get a good bound on the rank of any $\\langle n \\rangle$ for a specific $n$ we can use this to bound $\\omega$.\nWe can relate the a specific property of this tensor to our search for efficent ways of multiplying 2 matrices.\nStrassans Theorem $$ \\omega = \\lim_{n \\rightarrow \\infty} \\log_n (brk(\\langle n \\rangle))$$\nWhere $brk(t)$is the border rank of the tensor.\nIt has been shown that $rk(\\langle 2 \\rangle)) = 7$, this fact and the sub-multiplicity of tensor rank allows us to bound $\\omega \\leq \\log_2(7)$. This is the same bound we get by using Stattens Algorithm!\nThis is actually what Stratten\u0026rsquo;s Algorithm is doing. We are decomposing the $\\langle 2 \\rangle$ tensor into $7$ rank 1 tensors. The seemingly random matrixes represent this decomposition!\n","description":"","id":0,"section":"posts","tags":null,"title":"The Complexity of Matrix Multiplication","uri":"https://johnryan.tech/posts/matrix-multiplication/"},{"content":"Overview In this post we will be looking at the idea of attempting to calculate the expected value of the product of non independent normal distributions.\n$$ E\\left[\\prod_{i=1}^n X_i\\right], \\begin{bmatrix}X_1 \\ \\vdots \\ X_n\\end{bmatrix} \\sim N \\left(\\mu , \\Sigma \\right)$$\nThe simplest thing to do is kinda a cop-out, if we implicitly assume independence we get the following result.\n$$E\\left[\\prod_{i=1}^n X_i\\right] = \\prod_{i=1}^nE\\left[ X_i\\right] = \\prod_{i=1}^n \\mu_i $$\nThis can be useful as it is very simple to compute, but this assumption isn\u0026rsquo;t always valid.\nCalculating the intergral for the expectation directly analytically is difficult.\nWe will use the following theorem:\n(From wikipedia)\nIsserlis\u0026rsquo; Theorem\nIf $\\left(X_1,\\ldots,X_n\\right)$ is a zero-mean multivariate normal vector, then\n$$E\\left[X_1\\ldots X_n \\right] = \\sum_{p \\in P^2_n} \\prod_{{i,j} \\in P} E\\left[X_i,X_j\\right] = \\sum_{p \\in P^2_n} \\prod_{{i,j} \\in P} Cov\\left(X_i,X_j\\right)$$\nwhere the sum is over all the pairings of $\\left( 1, \\ldots, n\\right)$\nNote: This implies that if $n \\in 2\\mathbf{N} + 1$ , as we will have no valid pairings that our expectation will be 0.\nThis is close to what we want, but we unfortunely don\u0026rsquo;t have a 0 mean normal vectors, we have arbitary finite means.\nWe define the function $h : T \\rightarrow R$, where $T$ is the indexing set for our random variables.\n$$h(S) = E \\left[\\prod_{i \\in S} X_i\\right] $$\nFor convinence we will also define\n$$H(\\emptyset) = 1 $$\nWe already know the solution when $card(S) = 1$\n$$ H({s}) = E\\left[X_s\\right] = \\mu_s$$\nWe will also define $I : T \\rightarrow R$\n$I\\left(S\\right) = \\sum_{p \\in S^2} \\prod_{{i,j} \\in P} Cov\\left(X_i,X_j\\right)$\nWe will be using the fact that translating does not effect the covarience.\nAgain for convinence we will also define\n$$I(\\emptyset) = 1 $$\n$card(T) = n$\nWe will refer to the elements of T as ${1,\\ldots,n}$\nLet $n \\in 2\\mathbf{N}$\n$$\\begin{aligned} E\\left[\\left(X_1 - \\mu_1 \\right) \\ldots \\left(X_n - \\mu_n \\right)\\right] \u0026amp;= I\\left(T\\right)\\\\ E\\left[\\sum_{S \\subseteq T} \\prod_{i \\in T} \\mathbb{1}(i \\in S)X_i + \\mathbb{1}(i \\in T \\setminus S)\\left(-\\mu_i\\right) \\right] \u0026amp;= I\\left(T\\right)\\\\ \\sum_{S \\subseteq T} E\\left[\\prod_{i \\in T} \\mathbb{1}(i \\in S)X_i + \\mathbb{1}(i \\in T \\setminus S)\\left(-\\mu_i\\right) \\right] \u0026amp;= I\\left(T\\right)\\\\ \\sum_{S \\subseteq T} \\prod_{i \\in T \\setminus S} \\left(-\\mu_i\\right)E\\left[\\prod_{i \\in S} X_i \\right] \u0026amp;= I\\left(T\\right)\\\\ \\sum_{S \\subseteq T} \\prod_{i \\in T \\setminus S} \\left(-\\mu_i\\right)H(S) \u0026amp;= I\\left(T\\right)\\\\ H(T) + \\sum_{S \\subset T} \\prod_{i \\in T \\setminus S} \\left(-\\mu_i\\right)H(S) \u0026amp;= I\\left(T\\right)\\\\ H(T) = I\\left(T\\right) - \\sum_{S \\subset T} \\prod_{i \\in T \\setminus S} \\left(-\\mu_i\\right)H(S)\\\\ \\end{aligned}$$\nLet $n \\in 2\\mathbf{N} + 1$\n$$H(T) = - \\sum_{S \\subset T} \\prod_{i \\in T \\setminus S} \\left(-\\mu_i\\right)H(S)$$\nWe replaced $I(T)$ with $0$ as mentioned previously.\nThis recursive definition only relies on calls to smaller sets and we have defined the base cases so this will terminate.\nNow we wish to solve this recurrence relation.\nOur base cases are defined.\nLets calculate $H(T)$ with $T = {1,2}$.\n$$\\begin{aligned} H(T) \u0026amp;= I(T) - \\sum_{S \\subset T} \\prod_{i \\in T \\setminus S} \\left(-\\mu_i\\right)H(S) \\\\ H({1,2}) \u0026amp;= I({1,2}) - \\left( \\left(-\\mu_1\\right)H({2}) + \\left(-\\mu_2\\right)H({1} + \\mu_1 \\mu_2) \\right)\\\\ H({1,2}) \u0026amp;= I({1,2}) + \\mu_1 \\mu_2 \\\\ \\end{aligned}$$\nWe have an inductive hypothesis.\n$$H \\left(T \\right) = \\sum_{S \\subseteq T} \\left(\\prod_{i \\in T \\setminus S} \\mu_i \\right) I\\left(S\\right)$$\nWe have already seen that this is satisified for sets of size 1,2.\nNow for the inductive steps.\nCase 1: $n \\in 2\\mathbb{N}$\n$$\\begin{aligned} H(T) \u0026amp;= I(T) - \\sum_{S \\subset T} \\prod_{i \\in T \\setminus S} \\left(-\\mu_i\\right)H(S) \\\\ \u0026amp;= I(T) - \\sum_{S \\subset T} \\prod_{i \\in T \\setminus S} \\left(-\\mu_i\\right)\\sum_{D \\subseteq S} \\left(\\prod_{i \\in S \\setminus D} \\mu_i \\right) I\\left(D\\right) \\\\ \u0026amp;= I(T) - \\sum_{S \\subset T} \\left(\\prod_{i \\in T \\setminus S} \\left(-\\mu_i\\right) \\right)\\sum_{D \\subseteq S} \\left(\\prod_{i \\in S \\setminus D} \\mu_i \\right) I\\left(D\\right) \\\\ \u0026amp;= I(T) - \\sum_{S \\subset T} \\sum_{D \\subseteq S} \\left(\\prod_{i \\in T \\setminus S} \\left(-\\mu_i\\right) \\right)\\left(\\prod_{i \\in S \\setminus D} \\mu_i \\right) I\\left(D\\right) \\\\ \u0026amp;= I(T) - \\sum_{S \\subset T} \\sum_{D \\subseteq S} (-1)^{card(T \\setminus S)}\\left(\\prod_{i \\in T \\setminus S} \\mu_i \\right)\\left(\\prod_{i \\in S \\setminus D} \\mu_i \\right) I\\left(D\\right) \\\\ \u0026amp;= I(T) - \\sum_{S \\subset T} \\sum_{D \\subseteq S} (-1)^{card(T \\setminus S)}\\left(\\prod_{i \\in T \\setminus D} \\mu_i \\right) I\\left(D\\right) \\\\ \\end{aligned}$$\nIn our nested summation we have $(-1)^{card(T \\setminus S)}$ times a term that only depends on $D$ and $T$ and not $S$.\nAs $S \\subset T$, $1 \\leq card(T \\setminus S) \\leq n$\nFor a fixed $T$ and $D$, with $card(D) = m$.\n$$ \\begin{aligned} H(T) \u0026amp;= I(T) - \\sum_{D \\subset T} \\sum_{i=1}^{n-m} \\binom{n-m}{i}(-1)^{i}\\left(\\prod_{i \\in T \\setminus D} \\mu_i \\right) I\\left(D\\right) \\\\ H(T) \u0026amp;= I(T) - \\sum_{D \\subset T} (-1)\\left(\\prod_{i \\in T \\setminus D} \\mu_i \\right) I\\left(D\\right) \\\\ H(T) \u0026amp;= I(T) + \\sum_{D \\subset T} \\left(\\prod_{i \\in T \\setminus D} \\mu_i \\right) I\\left(D\\right) \\\\ H(T) \u0026amp;= \\sum_{D \\subseteq T} \\left(\\prod_{i \\in T \\setminus D} \\mu_i \\right) I\\left(D\\right) \\\\ \\end{aligned}$$\nCase 2: $n \\in 2\\mathbb{N} + 1$\n$$\\begin{aligned} H(T) \u0026amp;= - \\sum_{S \\subset T} \\prod_{i \\in T \\setminus S} \\left(-\\mu_i\\right)H(S)\\\\ H(T) \u0026amp;= - \\sum_{S \\subset T} \\sum_{D \\subseteq S} (-1)^{card(T \\setminus S)}\\left(\\prod_{i \\in T \\setminus D} \\mu_i \\right) I\\left(D\\right) \\\\ \u0026amp;= - \\sum_{D \\subset T} \\sum_{i=1}^{n-m}\\binom{n-m}{i} (-1)^{i}\\left(\\prod_{i \\in T \\setminus D} \\mu_i \\right) I\\left(D\\right) \\\\ \u0026amp;= - \\sum_{D \\subset T} (-1)\\left(\\prod_{i \\in T \\setminus D} \\mu_i \\right) I\\left(D\\right) \\\\ \u0026amp;= \\sum_{D \\subset T} \\left(\\prod_{i \\in T \\setminus D} \\mu_i \\right) I\\left(D\\right) \\\\ \u0026amp;= \\sum_{D \\subseteq T} \\left(\\prod_{i \\in T \\setminus D} \\mu_i \\right) I\\left(D\\right) \\\\ \\end{aligned}$$\nWe can do this as $I(T) = 0$, due to the size of the set being odd.\nThis completes the proof by induction.\n$$H \\left(T \\right) = \\sum_{S \\subseteq T} \\left(\\prod_{i \\in T \\setminus S} \\mu_i \\right) I\\left(S\\right)$$\nWe now have an explicit form for the expected value.\nThis however is quite expensive to compute, The outer summation makes this an exponentially expensive calculation in the size of $T$.\n$$\\begin{aligned} H \\left(T \\right) \u0026amp;= \\sum_{S \\subseteq T} \\left(\\prod_{i \\in T \\setminus S} \\mu_i \\right) \\left(\\sum_{p \\in S^2} \\prod_{{i,j} \\in P} Cov\\left(X_i,X_j\\right) \\right) \\ \\end{aligned}$$\nWe can see the if $\\forall i:\\mu_i = 0$, the summation will collapse except for when $S = T$, and we recover Isserlis Theorem.\n","description":"","id":1,"section":"posts","tags":["machine learning"],"title":"Multivariate Normal Expectation","uri":"https://johnryan.tech/posts/mvn-likelihood/"},{"content":"TLDR Gaussian Processes can be used to predict unknown values of a function, using known points and some function which specifies how the values of different points should relate.\nDefinition (From wikipedia)\nA time continuous schochastic process $\\left{ X_t ; t \\in T \\right}$ is Gaussian iff for every finite set of indices $t_1,\\ldots ,t_k \\in T$\n$$X_{t_1,\\ldots, t_k} = \\left(X_{t_1} , \\ldots, X_{t_k} \\right)$$\nis a multivariate Gaussian.\nWe can almost view this as a infinite dimensional Gaussian.\nWhy? When we are attempting a machine learning problem we are generally attempting to learn specific values (parameters) of some function that we have decided on.\nEg you might use a feed-forward neural network of a specific size and activations, but you need to learn the weights of this function. By doing this we are adding significant priors to our beliefs, some we might not even realise.\nThe idea in Gaussian Processes is a bit different, we don\u0026rsquo;t know the function and are going to try and learn the function itself, not the parameters of a pre chosen function.\nThere\u0026rsquo;s clearly an issue with this approach, without some other constraints this isn\u0026rsquo;t going to be very useful. The span of functions that could be correct in theory can be weird and wonderful.\nSo we would like to add some prior belief to the space of functions we want to consider.\nThe idea behind using Gaussian Processes to model a function is for the output of the function for a specific input is modelled as a random variable which is normally distributed.\nHowever these random variables don\u0026rsquo;t have to be independent. We decide the prior covariances according to a kernel function.\nWe also need a prior for the means of each of these variables.\nWe now have an prior for values of a function at a set of points.\nIf we have some data-points we can condition over knowing their values $\\left((x_i, y_i), \\ldots \\right)$ to get a posterior distribution over the remainder of them.\nWe can model epistemic uncertainty of measurements by not conditioning over knowing the exact value but by conditioning over $x_i$ being distributed via $N(y_i, \\sigma^2)$ for some noise parameter $\\sigma$.\nTechnical Details - Todo ","description":"","id":2,"section":"posts","tags":["machine learning"],"title":"Gaussian Processes","uri":"https://johnryan.tech/posts/gaussian-processes/"},{"content":"This is going to be possibly the first in a series of posts primarily aimed at for leaving cert students who are currently missing out on school due to COVID-19 but still have exams.\nThese will be aimed at helping you to understand a concept so you don\u0026rsquo;t need to rely on rote learning (I always found that quite difficult personally)\nProof by Induction Proofs are core to maths and are a reasonably reliable part of the Leaving Cert Maths papers.\nThe most varied type of proof that you will get are the proofs by induction.\nIntuition Often when induction is being explained to students (or being rushed through :/) the idea of induction itself isn\u0026rsquo;t properly separated from a specific way of writing an inductive proof. The idea itself is super powerful and actually quite simple.\nLets take the following as an example. We want to show that something is true\n$1 + 3 + \\ldots + (2n-1) = n^2$ for all the natural numbers.\nOr we could say it like this if you like using the more formal maths notation\n$\\forall n \\in \\mathbb{N}:1 + 3 + \\ldots + (2n-1) = n^2$\nHow can we show this for all of the natural numbers? We know that there is an infinite number of them, so even if we tested for the first $100$ numbers there could be a bigger number that we just haven\u0026rsquo;t tested that it could be false for.\nInduction Note: As far as I can tell from checking the syllabus weak induction is the only form of induction needed for the leaving certificate. From here on out I will just call it induction.\nThe idea behind induction is that we solve the problem in 2 different parts.\nWe have the statement $P(n)$, which we are trying to show is true. For example if we are using the example above:\n$P(n) : 1 + 3 + \\ldots + (2n-1) = n^2$\nFor a specific value of $n$ this statement is going to be true or false. And this can simply be checked by calculating both sides and checking if they are equal.\nIn the first part we come up with a proof for showing that if $P$ is true for a specific number $k$ that we can show that it is true for $k+1$.\nOr in more formal maths terms we want a proof of $P(k) \\implies P(k+1)$\nLets call this \u0026ldquo;sub proof\u0026rdquo; the inductive step.\nIn the second part we show that our problem is true for a specific number $s$. This is showing that $P(s)$ is true. This can be as simple as calculating what the statement says is equal and just checking. Lets call this part of the proof base case.\nThen we can combine these 2 parts to show that $P(s)$ is true for all numbers $\\geq s$.\nUsing the fact that $P(s)$ is true and $P(k) \\implies P(k+1)$ we get that $P(s+1)$ is true. We can repeat this to get that $P(s+2),P(s+3),\\ldots$ are all true.\nWe can visualise this with the following diagram.\nEach dot is a something we have shown is true, the arrows indicate how we have show it to be true.\ndigraph induction { rankdir=LR; node [shape = circle]; b [label=\u0026quot;P(s)\u0026quot;] b1 [label=\u0026quot;P(s+1)\u0026quot;] b2 [label=\u0026quot;P(s+2)\u0026quot;] b -\u0026gt; b1 [ label = \u0026quot;Inductive Step\u0026quot; ]; b1 -\u0026gt; b2 [ label = \u0026quot;Inductive Step\u0026quot; ]; node [shape = point]; b2 -\u0026gt; b3[ label = \u0026quot;I.S\u0026quot; ]; a -\u0026gt; b [ label = \u0026quot;Base Case\u0026quot; ]; } This is what induction is, the particular way you write it shouldn\u0026rsquo;t matter too much.\nNotice how the arrows follow a nice line with no loops? This makes the argument sensible, if you make a mistake and in your argument and the implication arrows form loops your \u0026ldquo;argument is going around in circles\u0026rdquo;.\nAside: Not needed for leaving certificate but strong induction is when that graph we just drew would be more complicated, but still with no loops. Imagine if your proof for the next case to be true relied on both of the previous values being true.\ndigraph induction { rankdir=LR; node [shape = circle]; b [label=\u0026quot;P(s)\u0026quot;] b -\u0026gt; b2 [ label = \u0026quot;Inductive Step\u0026quot; ]; b1 [label=\u0026quot;P(s+1)\u0026quot;] b2 [label=\u0026quot;P(s+2)\u0026quot;] b1 -\u0026gt; b2 [ label = \u0026quot;Inductive Step\u0026quot; ]; node [shape = point]; b2 -\u0026gt; b3[ label = \u0026quot;Inductive Step\u0026quot; ]; a -\u0026gt; b [ label = \u0026quot;Base Case\u0026quot; ]; c -\u0026gt; b1 [ label = \u0026quot;Base Case\u0026quot; ]; } How to write an inductive proof With the understanding of what induction is and how it works under our belts lets see how to write a proof. While the induction is reasonably straightforward, it builds quite a bit on writing of other types of proofs and algebra so it might be worth revising those if you are finding this part tough.\nWe will use the example problem from above in this example.\nStep 1: Write down you problem While this might seem stupid to put as its own step this is where I see most people go wrong, you don\u0026rsquo;t want to be stuck on a problem because you took it down wrong.\nTo prove: $\\forall n \\in \\mathbb{N}:1 + 3 + \\ldots + (2n-1) = n^2$\nStep 2: The inductive step We assume that $P(k)$ is true, from this we need to show that $P(k+1)$ is true.\n$$1 + 3 + \\ldots + (2k-1) = k^2 \\tag{P(k)}$$\n$$\\begin{aligned} 1 + 3 + \\ldots + (2k-1) + (2k+1) \u0026amp;= 1 + 3 + \\ldots + (2k-1) + (2k+1) \\\\ 1 + 3 + \\ldots + (2k+1) \u0026amp;= k^2 + (2k+1) \\\\ 1 + 3 + \\ldots + \\left(2(k+1)-1\\right) \u0026amp;= (k+1)^2 \\\\ \\end{aligned}$$\nWe have now shown that $P(k) \\implies P(k+1)$, we have now completed the inductive step.\nStep 3: The base case We simply need to test that $P(1)$ is true.\n$$1 = 1^2 \\tag{True} $$\nWe have now completed the base case.\nStep 4: Putting it together As induction is widely understood we don\u0026rsquo;t need to explain it when we use it.\n$P(1)$ is true and $P(k) \\implies P(k+1)$ therefore by induction $P(n)$ is true for all $n \\in \\mathbb{N}$\nConclusion I hope that this has been helpful to you and if you have any questions or any other topics you would like me to cover you can message me at @john_pryan on twitter.\n","description":"","id":3,"section":"posts","tags":["leaving cert"],"title":"Induction for the Leaving Cert","uri":"https://johnryan.tech/posts/induction-lc/"}]