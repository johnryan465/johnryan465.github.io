[{"content":"TLDR Gaussian Processes can be used to predict unknown values of a function, using known points and some function which specifies how the values of different points should relate.\nDefinition (From wikipedia)\nA time continuous schochastic process $\\left{ X_t ; t \\in T \\right}$ is Gaussian iff for every finite set of indices $t_1,\\ldots ,t_k \\in T$\n$$X_{t_1,\\ldots, t_k} = \\left(X_{t_1} , \\ldots, X_{t_k} \\right)$$\nis a multivariate Gaussian.\nWe can almost view this as a infinite dimensional Gaussian.\nWhy? When we are attempting a machine learning problem we are generally attempting to learn specific values (parameters) of some function that we have decided on.\nEg you might use a feed-forward neural network of a specific size and activations, but you need to learn the weights of this function. By doing this we are adding significant priors to our beliefs, some we might not even realise.\nThe idea in Gaussian Processes is a bit different, we don\u0026rsquo;t know the function and are going to try and learn the function itself, not the parameters of a pre chosen function.\nThere\u0026rsquo;s clearly an issue with this approach, without some other constraints this isn\u0026rsquo;t going to be very useful. The span of functions that could be correct in theory can be weird and wonderful.\nSo we would like to add some prior belief to the space of functions we want to consider.\nThe idea behind using Gaussian Processes to model a function is for the output of the function for a specific input is modelled as a random variable which is normally distributed.\nHowever these random variables don\u0026rsquo;t have to be independent. We decide the prior covariances according to a kernel function.\nWe also need a prior for the means of each of these variables.\nWe now have an prior for values of a function at a set of points.\nIf we have some data-points we can condition over knowing their values $\\left((x_i, y_i), \\ldots \\right)$ to get a posterior distribution over the remainder of them.\nWe can model epistemic uncertainty of measurements by not conditioning over knowing the exact value but by conditioning over $x_i$ being distributed via $N(y_i, \\sigma^2)$ for some noise parameter $\\sigma$.\nTechnical Details - Todo ","description":"","id":0,"section":"posts","tags":["machine learning"],"title":"Gaussian Processes","uri":"https://johnryan.tech/posts/gaussian-processes/"},{"content":"This is going to be possibly the first in a series of posts primarily aimed at for leaving cert students who are currently missing out on school due to COVID-19 but still have exams.\nThese will be aimed at helping you to understand a concept so you don\u0026rsquo;t need to rely on rote learning (I always found that quite difficult personally)\nProof by Induction Proofs are core to maths and are a reasonably reliable part of the Leaving Cert Maths papers.\nThe most varied type of proof that you will get are the proofs by induction.\nIntuition Often when induction is being explained to students (or being rushed through :/) the idea of induction itself isn\u0026rsquo;t properly separated from a specific way of writing an inductive proof. The idea itself is super powerful and actually quite simple.\nLets take the following as an example. We want to show that something is true\n$1 + 3 + \\ldots + (2n-1) = n^2$ for all the natural numbers.\nOr we could say it like this if you like using the more formal maths notation\n$\\forall n \\in \\mathbb{N}:1 + 3 + \\ldots + (2n-1) = n^2$\nHow can we show this for all of the natural numbers? We know that there is an infinite number of them, so even if we tested for the first $100$ numbers there could be a bigger number that we just haven\u0026rsquo;t tested that it could be false for.\nInduction Note: As far as I can tell from checking the syllabus weak induction is the only form of induction needed for the leaving certificate. From here on out I will just call it induction.\nThe idea behind induction is that we solve the problem in 2 different parts.\nWe have the statement $P(n)$, which we are trying to show is true. For example if we are using the example above:\n$P(n) : 1 + 3 + \\ldots + (2n-1) = n^2$\nFor a specific value of $n$ this statement is going to be true or false. And this can simply be checked by calculating both sides and checking if they are equal.\nIn the first part we come up with a proof for showing that if $P$ is true for a specific number $k$ that we can show that it is true for $k+1$.\nOr in more formal maths terms we want a proof of $P(k) \\implies P(k+1)$\nLets call this \u0026ldquo;sub proof\u0026rdquo; the inductive step.\nIn the second part we show that our problem is true for a specific number $s$. This is showing that $P(s)$ is true. This can be as simple as calculating what the statement says is equal and just checking. Lets call this part of the proof base case.\nThen we can combine these 2 parts to show that $P(s)$ is true for all numbers $\\geq s$.\nUsing the fact that $P(s)$ is true and $P(k) \\implies P(k+1)$ we get that $P(s+1)$ is true. We can repeat this to get that $P(s+2),P(s+3),\\ldots$ are all true.\nWe can visualise this with the following diagram.\nEach dot is a something we have shown is true, the arrows indicate how we have show it to be true.\ndigraph induction { rankdir=LR; node [shape = circle]; b [label=\u0026quot;P(s)\u0026quot;] b1 [label=\u0026quot;P(s+1)\u0026quot;] b2 [label=\u0026quot;P(s+2)\u0026quot;] b -\u0026gt; b1 [ label = \u0026quot;Inductive Step\u0026quot; ]; b1 -\u0026gt; b2 [ label = \u0026quot;Inductive Step\u0026quot; ]; node [shape = point]; b2 -\u0026gt; b3[ label = \u0026quot;I.S\u0026quot; ]; a -\u0026gt; b [ label = \u0026quot;Base Case\u0026quot; ]; } This is what induction is, the particular way you write it shouldn\u0026rsquo;t matter too much.\nNotice how the arrows follow a nice line with no loops? This makes the argument sensible, if you make a mistake and in your argument and the implication arrows form loops your \u0026ldquo;argument is going around in circles\u0026rdquo;.\nAside: Not needed for leaving certificate but strong induction is when that graph we just drew would be more complicated, but still with no loops. Imagine if your proof for the next case to be true relied on both of the previous values being true.\ndigraph induction { rankdir=LR; node [shape = circle]; b [label=\u0026quot;P(s)\u0026quot;] b -\u0026gt; b2 [ label = \u0026quot;Inductive Step\u0026quot; ]; b1 [label=\u0026quot;P(s+1)\u0026quot;] b2 [label=\u0026quot;P(s+2)\u0026quot;] b1 -\u0026gt; b2 [ label = \u0026quot;Inductive Step\u0026quot; ]; node [shape = point]; b2 -\u0026gt; b3[ label = \u0026quot;Inductive Step\u0026quot; ]; a -\u0026gt; b [ label = \u0026quot;Base Case\u0026quot; ]; c -\u0026gt; b1 [ label = \u0026quot;Base Case\u0026quot; ]; } How to write an inductive proof With the understanding of what induction is and how it works under our belts lets see how to write a proof. While the induction is reasonably straightforward, it builds quite a bit on writing of other types of proofs and algebra so it might be worth revising those if you are finding this part tough.\nWe will use the example problem from above in this example.\nStep 1: Write down you problem While this might seem stupid to put as its own step this is where I see most people go wrong, you don\u0026rsquo;t want to be stuck on a problem because you took it down wrong.\nTo prove: $\\forall n \\in \\mathbb{N}:1 + 3 + \\ldots + (2n-1) = n^2$\nStep 2: The inductive step We assume that $P(k)$ is true, from this we need to show that $P(k+1)$ is true.\n$$1 + 3 + \\ldots + (2k-1) = k^2 \\tag{P(k)}$$\n$$\\begin{aligned} 1 + 3 + \\ldots + (2k-1) + (2k+1) \u0026amp;= 1 + 3 + \\ldots + (2k-1) + (2k+1) \\\\ 1 + 3 + \\ldots + (2k+1) \u0026amp;= k^2 + (2k+1) \\\\ 1 + 3 + \\ldots + \\left(2(k+1)-1\\right) \u0026amp;= (k+1)^2 \\\\ \\end{aligned}$$\nWe have now shown that $P(k) \\implies P(k+1)$, we have now completed the inductive step.\nStep 3: The base case We simply need to test that $P(1)$ is true.\n$$1 = 1^2 \\tag{True} $$\nWe have now completed the base case.\nStep 4: Putting it together As induction is widely understood we don\u0026rsquo;t need to explain it when we use it.\n$P(1)$ is true and $P(k) \\implies P(k+1)$ therefore by induction $P(n)$ is true for all $n \\in \\mathbb{N}$\nConclusion I hope that this has been helpful to you and if you have any questions or any other topics you would like me to cover you can message me at @john_pryan on twitter.\n","description":"","id":1,"section":"posts","tags":["leaving cert"],"title":"Induction for the Leaving Cert","uri":"https://johnryan.tech/posts/induction-lc/"},{"content":"Overview In this post we will be looking at the idea of attempting to calculate the expected value of the product of non independent normal distributions.\n$$ E\\left[\\prod_{i=1}^n X_i\\right], \\begin{bmatrix}X_1 \\ \\vdots \\ X_n\\end{bmatrix} \\sim N \\left(\\mu , \\Sigma \\right)$$\nThe simplest thing to do is kinda a cop-out, if we implicitly assume independence we get the following result.\n$$E\\left[\\prod_{i=1}^n X_i\\right] = \\prod_{i=1}^nE\\left[ X_i\\right] = \\prod_{i=1}^n \\mu_i $$\nThis can be useful as it is very simple to compute, but this assumption isn\u0026rsquo;t always valid.\nCalculating the intergral for the expectation directly analytically is difficult.\nWe will use the following theorem:\n(From wikipedia)\nIsserlis\u0026rsquo; Theorem\nIf $\\left(X_1,\\ldots,X_n\\right)$ is a zero-mean multivariate normal vector, then\n$$E\\left[X_1\\ldots X_n \\right] = \\sum_{p \\in P^2_n} \\prod_{{i,j} \\in P} E\\left[X_i,X_j\\right] = \\sum_{p \\in P^2_n} \\prod_{{i,j} \\in P} Cov\\left(X_i,X_j\\right)$$\nwhere the sum is over all the pairings of $\\left{ 1, \\ldots, n\\right}$\nNote: This implies that if $n \\in 2\\mathbf{N} + 1$, as we will have no valid pairings that our expectation will be 0.\nThis is close to what we want, but we unfortunely don\u0026rsquo;t have a 0 mean normal vectors, we have arbitary finite means.\nWe define the function $h : T \\rightarrow R$, where $T$ is the indexing set for our random variables.\n$$h(S) = E \\left[\\prod_{i \\in S} X_i\\right] $$\nFor convinence we will also define\n$$H(\\emptyset) = 1 $$\nWe already know the solution when $#S = 1$\n$$ H({s}) = E\\left[X_s\\right] = \\mu_s$$\nWe will also define $I : T \\rightarrow R$\n$I\\left(S\\right) = \\sum_{p \\in S^2} \\prod_{{i,j} \\in P} Cov\\left(X_i,X_j\\right)$\nWe will be using the fact that translating does not effect the covarience.\nAgain for convinence we will also define\n$$I(\\emptyset) = 1 $$\n$#T = n$\nWe will refer to the elements of T as ${1,\\ldots,n}$\nLet $n \\in 2\\mathbf{N}$\n$$\n\\begin{aligned}\nE\\left[\\left(X_1 - \\mu_1 \\right) \\ldots \\left(X_n - \\mu_n \\right)\\right] \u0026amp;= I\\left(T\\right)\\\nE\\left[\\sum_{S \\subseteq T} \\prod_{i \\in T} \\mathbb{1}(i \\in S)X_i + \\mathbb{1}(i \\notin S)\\left(-\\mu_i\\right) \\right] \u0026amp;= I\\left(T\\right)\\\n\\sum_{S \\subseteq T} E\\left[\\prod_{i \\in T} \\mathbb{1}(i \\in S)X_i + \\mathbb{1}(i \\notin S)\\left(-\\mu_i\\right) \\right] \u0026amp;= I\\left(T\\right)\\\n\\sum_{S \\subseteq T} \\prod_{i \\in T \\setminus S} \\left(-\\mu_i\\right)E\\left[\\prod_{i \\in S} X_i \\right] \u0026amp;= I\\left(T\\right)\\\n\\sum_{S \\subseteq T} \\prod_{i \\in T \\setminus S} \\left(-\\mu_i\\right)H(S) \u0026amp;= I\\left(T\\right)\\\nH(T) + \\sum_{S \\subset T} \\prod_{i \\in T \\setminus S} \\left(-\\mu_i\\right)H(S) \u0026amp;= I\\left(T\\right)\\\nH(T) = I\\left(T\\right) - \\sum_{S \\subset T} \\prod_{i \\in T \\setminus S} \\left(-\\mu_i\\right)H(S)\\\n\\end{aligned}\n$$\nLet $n \\in 2\\mathbf{N} + 1$\n$$H(T) = - \\sum_{S \\subset T} \\prod_{i \\in T \\setminus S} \\left(-\\mu_i\\right)H(S)\\\n$$\nWe replaced $I(T)$ with $0$ as mentioned previously.\nThis recursive definition only relies on calls to smaller sets and we have defined the base cases so this will terminate.\nNow we wish to solve this recurrence relation.\nOur base cases are defined.\nLets calculate $H(T)$ with $T = {1,2}$.\n$$\n\\begin{aligned}\nH(T) \u0026amp;= I(T) - \\sum_{S \\subset T} \\prod_{i \\in T \\setminus S} \\left(-\\mu_i\\right)H(S) \\\nH({1,2}) \u0026amp;= I({1,2}) - \\left( \\left(-\\mu_1\\right)H({2}) + \\left(-\\mu_2\\right)H({1} + \\mu_1 \\mu_2) \\right)\\\nH({1,2}) \u0026amp;= I({1,2}) + \\mu_1 \\mu_2\\\n\\end{aligned}\n$$\nWe have an inductive hypothesis.\n$$H \\left(T \\right) = \\sum_{S \\subseteq T} \\left(\\prod_{i \\in T \\setminus S} \\mu_i \\right) I\\left(S\\right)$$\nWe have already seen that this is satisified for sets of size 1,2.\nNow for the inductive steps.\nCase 1: $n \\in 2\\mathbb{N}$\n$$\n\\begin{aligned}\nH(T) \u0026amp;= I(T) - \\sum_{S \\subset T} \\prod_{i \\in T \\setminus S} \\left(-\\mu_i\\right)H(S) \\\n\u0026amp;= I(T) - \\sum_{S \\subset T} \\prod_{i \\in T \\setminus S} \\left(-\\mu_i\\right)\\sum_{D \\subseteq S} \\left(\\prod_{i \\in S \\setminus D} \\mu_i \\right) I\\left(D\\right) \\\n\u0026amp;= I(T) - \\sum_{S \\subset T} \\left(\\prod_{i \\in T \\setminus S} \\left(-\\mu_i\\right) \\right)\\sum_{D \\subseteq S} \\left(\\prod_{i \\in S \\setminus D} \\mu_i \\right) I\\left(D\\right) \\\n\u0026amp;= I(T) - \\sum_{S \\subset T} \\sum_{D \\subseteq S} \\left(\\prod_{i \\in T \\setminus S} \\left(-\\mu_i\\right) \\right)\\left(\\prod_{i \\in S \\setminus D} \\mu_i \\right) I\\left(D\\right) \\\n\u0026amp;= I(T) - \\sum_{S \\subset T} \\sum_{D \\subseteq S} (-1)^{#T \\setminus S }\\left(\\prod_{i \\in T \\setminus S} \\mu_i \\right)\\left(\\prod_{i \\in S \\setminus D} \\mu_i \\right) I\\left(D\\right) \\\n\u0026amp;= I(T) - \\sum_{S \\subset T} \\sum_{D \\subseteq S} (-1)^{#T \\setminus S }\\left(\\prod_{i \\in T \\setminus D} \\mu_i \\right) I\\left(D\\right) \\\n\\end{aligned}\n$$\nIn our nested summation we have $(-1)^{#T \\setminus S}$ times a term that only depends on $D$ and $T$ and not $S$.\nAs $S \\subset T$, $1 \\leq #T \\setminus S \\leq n$\nFor a fixed $T$ and $D$, with $#D = m$.\n$$\n\\begin{aligned}\nH(T) \u0026amp;= I(T) - \\sum_{D \\subset T} \\sum_{i=1}^{n-m} \\binom{n-m}{i}(-1)^{i}\\left(\\prod_{i \\in T \\setminus D} \\mu_i \\right) I\\left(D\\right) \\\nH(T) \u0026amp;= I(T) - \\sum_{D \\subset T}\n(-1)\n\\left(\\prod_{i \\in T \\setminus D} \\mu_i \\right) I\\left(D\\right) \\\nH(T) \u0026amp;= I(T) + \\sum_{D \\subset T}\n\\left(\\prod_{i \\in T \\setminus D} \\mu_i \\right) I\\left(D\\right) \\\nH(T) \u0026amp;= \\sum_{D \\subseteq T} \\left(\\prod_{i \\in T \\setminus D} \\mu_i \\right) I\\left(D\\right) \\\n\\end{aligned}\n$$\nCase 2: $n \\in 2\\mathbb{N} + 1$\n$$\n\\begin{aligned}\nH(T) \u0026amp;= - \\sum_{S \\subset T} \\prod_{i \\in T \\setminus S} \\left(-\\mu_i\\right)H(S)\\\nH(T) \u0026amp;= - \\sum_{S \\subset T} \\sum_{D \\subseteq S} (-1)^{#T \\setminus S }\\left(\\prod_{i \\in T \\setminus D} \\mu_i \\right) I\\left(D\\right) \\\n\u0026amp;= - \\sum_{D \\subset T} \\sum_{i=1}^{n-m}\\binom{n-m}{i} (-1)^{i}\\left(\\prod_{i \\in T \\setminus D} \\mu_i \\right) I\\left(D\\right) \\\n\u0026amp;= - \\sum_{D \\subset T} (-1)\\left(\\prod_{i \\in T \\setminus D} \\mu_i \\right) I\\left(D\\right) \\\n\u0026amp;= \\sum_{D \\subset T} \\left(\\prod_{i \\in T \\setminus D} \\mu_i \\right) I\\left(D\\right) \\\n\u0026amp;= \\sum_{D \\subseteq T} \\left(\\prod_{i \\in T \\setminus D} \\mu_i \\right) I\\left(D\\right) \\\n\\end{aligned}\n$$\nWe can do this as $I(T) = 0$, due to the size of the set being odd.\nThis completes the proof by induction.\n$$H \\left(T \\right) = \\sum_{S \\subseteq T} \\left(\\prod_{i \\in T \\setminus S} \\mu_i \\right) I\\left(S\\right)$$\nWe now have an explicit form for the expected value.\nThis however is quite expensive to compute, The outer summation makes this an exponentially expensive calculation in the size of $T$.\n$$\n\\begin{aligned}\nH \\left(T \\right) \u0026amp;= \\sum_{S \\subseteq T} \\left(\\prod_{i \\in T \\setminus S} \\mu_i \\right) \\left(\\sum_{p \\in S^2} \\prod_{{i,j} \\in P} Cov\\left(X_i,X_j\\right) \\right) \\\n\\end{aligned}\n$$\nWe can see the if $\\forall i:\\mu_i = 0$, the summation will collapse except for when $S = T$, and we recover Isserlis Theorem.\n","description":"","id":2,"section":"posts","tags":["machine learning"],"title":"Multivariate Normal Expectation Maximisation","uri":"https://johnryan.tech/posts/mvn-likelihood/"}]