<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:sy="http://purl.org/rss/1.0/modules/syndication/" xmlns:media="http://search.yahoo.com/mrss/"><channel><title>johnryan.tech</title><link>https://johnryan.tech/</link><description>Recent content on johnryan.tech</description><generator>Hugo -- gohugo.io</generator><language>en</language><managingEditor>johnryan465@gmail.com (John Ryan)</managingEditor><webMaster>johnryan465@gmail.com (John Ryan)</webMaster><copyright>Â©{year}, All Rights Reserved</copyright><lastBuildDate>Tue, 19 May 2020 15:29:01 +0000</lastBuildDate><atom:link href="https://johnryan.tech/index.xml" rel="self" type="application/rss+xml"/><item><title>Gaussian Processes</title><link>https://johnryan.tech/posts/gaussian-processes/</link><pubDate>Tue, 19 May 2020 15:29:01 +0000</pubDate><author>johnryan465@gmail.com (John Ryan)</author><atom:modified>Tue, 19 May 2020 15:29:01 +0000</atom:modified><guid>https://johnryan.tech/posts/gaussian-processes/</guid><description>TLDR Gaussian Processes can be used to predict unknown values of a function, using known points and some function which specifies how the values of different points should relate.
Definition (From wikipedia)
A time continuous schochastic process $\left{ X_t ; t \in T \right}$ is Gaussian iff for every finite set of indices $t_1,\ldots ,t_k \in T$
$$X_{t_1,\ldots, t_k} = \left(X_{t_1} , \ldots, X_{t_k} \right)$$
is a multivariate Gaussian.
We can almost view this as a infinite dimensional Gaussian.</description><dc:creator>John Ryan</dc:creator><category>machine learning</category></item><item><title>Induction for the Leaving Cert</title><link>https://johnryan.tech/posts/induction-lc/</link><pubDate>Mon, 13 Apr 2020 14:07:33 +0000</pubDate><author>johnryan465@gmail.com (John Ryan)</author><atom:modified>Mon, 13 Apr 2020 14:07:33 +0000</atom:modified><guid>https://johnryan.tech/posts/induction-lc/</guid><description>This is going to be possibly the first in a series of posts primarily aimed at for leaving cert students who are currently missing out on school due to COVID-19 but still have exams.
These will be aimed at helping you to understand a concept so you don&amp;rsquo;t need to rely on rote learning (I always found that quite difficult personally)
Proof by Induction Proofs are core to maths and are a reasonably reliable part of the Leaving Cert Maths papers.</description><dc:creator>John Ryan</dc:creator><category>leaving cert</category></item><item><title>Multivariate Normal Expectation Maximisation</title><link>https://johnryan.tech/posts/mvn-likelihood/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><author>johnryan465@gmail.com (John Ryan)</author><guid>https://johnryan.tech/posts/mvn-likelihood/</guid><description>Overview In this post we will be looking at the idea of attempting to calculate the expected value of the product of non independent normal distributions.
$$ E\left[\prod_{i=1}^n X_i\right], \begin{bmatrix}X_1 \ \vdots \ X_n\end{bmatrix} \sim N \left(\mu , \Sigma \right)$$
The simplest thing to do is kinda a cop-out, if we implicitly assume independence we get the following result.
$$E\left[\prod_{i=1}^n X_i\right] = \prod_{i=1}^nE\left[ X_i\right] = \prod_{i=1}^n \mu_i $$
This can be useful as it is very simple to compute, but this assumption isn&amp;rsquo;t always valid.</description><dc:creator>John Ryan</dc:creator><category>machine learning</category></item></channel></rss>